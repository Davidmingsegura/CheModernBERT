{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "from transformers.utils import is_torch_available\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"ESOL\": (df_esol, \"label\"),\n",
    "    \"FreeSolv\"      : (df_freesolv,   \"label\"),\n",
    "    \"Lipophilicity\" : (df_lipo,       \"label\"),\n",
    "    \"QM9 (alpha)\"     : (df_qm9_alpha,  \"label\"),\n",
    "    \"QM9 (mu)\"        : (df_qm9_mu,      \"label\"),\n",
    "    \"QM9 (A)\"        : (df_qm9_A,      \"label\"),\n",
    "    \"QM9 (B)\"        : (df_qm9_B,      \"label\"),\n",
    "    \"QM9 (C)\"        : (df_qm9_C,      \"label\"),\n",
    "    \"QM9 (Homo)\"        : (df_qm9_homo,      \"label\"),\n",
    "    \"QM9 (Lumo)\"        : (df_qm9_lumo,      \"label\"),\n",
    "    \"QM9 (HL Gap)\"        : (df_qm9_gap,      \"label\"),\n",
    "    \"QM9 (r2)\"        : (df_qm9_r2,      \"label\"),\n",
    "    \"QM9 (zpve)\"        : (df_qm9_zpve,      \"label\"),\n",
    "    \"QM9 (u0)\"        : (df_qm9_u0,      \"label\"),\n",
    "    \"QM9 (u298)\"        : (df_qm9_u298,      \"label\"),\n",
    "    \"QM9 (h298)\"        : (df_qm9_h298,      \"label\"),\n",
    "    \"QM9 (g298)\"        : (df_qm9_g298,      \"label\"),\n",
    "    \"QM9 (CV)\"        : (df_qm9_cv,      \"label\"),\n",
    "    \"QM9 (u0 atom)\"        : (df_qm9_u0_atom,  \"label\"),\n",
    "    \"QM9 (u298 atom)\"        : (df_qm9_u298_atom,      \"label\"),\n",
    "    \"QM9 (h298 atom)\"        : (df_qm9_h298_atom,      \"label\"),\n",
    "    \"QM9 (g298 atom)\"        : (df_qm9_g298_atom,      \"label\"),\n",
    "    \"MP tryg.\" : (df_mptri, \"label\"),\n",
    "    \"MP Molecules\": (df_mp, \"label\"),\n",
    "    \"Dyn. Visc.\": (df_dynvisc, \"label\"),\n",
    "    \"Free E. cyclo\": (df_cyclo, \"label\"),\n",
    "    \"Monomer Densities\": (df_Densities, \"label\"),\n",
    "    \"Monomer Rgyr\": (df_Rgyr, \"label\"),\n",
    "    \"Monomer Tg\": (df_Tg, \"label\"),\n",
    "    \"Monomer Ecoh\": (df_Ecoh, \"label\"),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dirs = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"DeepChem/ChemBERTa-77M-MLM\",\n",
    "    'DeepChem/ChemBERTa-77M-MTR',\n",
    "    'ibm-research/MoLFormer-XL-both-10pct',\n",
    "    \"allenai/scibert_scivocab_uncased\",\n",
    "    \"allenai/scibert_scivocab_cased\",\n",
    "    \"answerdotai/ModernBERT-base\",\n",
    "    # \"/home/david/modernbert_chemistry/fineweb/fine-web-modernbert-base-8192-multi-tok-1n4g-no-tags\",\n",
    "    # \"/home/david/modernbert_chemistry/fineweb/fine-web-modernbert-base-8192-multi-tok-new-procedure-500k-notags\",\n",
    "    # \"/home/david/modernbert_chemistry/fineweb/fine-web-modernbert-base-8192-multi-tok-new-procedure-1M5-notags\",\n",
    "    # \"/home/david/modernbert_chemistry/fineweb/fine-web-modernbert-base-8192-multi-tok-new-procedure-10-epochs-notags\",\n",
    "    # \"/home/david/modernbert_chemistry/fineweb/fine-web-modernbert-base-8192-multi-tok-new-procedure-20-epochs-notags\",\n",
    "    # \"/home/david/modernbert_chemistry/fineweb/fine-web-modernbert-base-8192-multi-tok-new-procedure-30-epochs-notags\",\n",
    "    # \"/home/david/modernbert_chemistry/fineweb/fine-web-modernbert-base-8192-multi-tok-new-procedure-40-epochs-notags\",\n",
    "    # \"/home/david/modernbert_chemistry/fineweb/fine-web-modernbert-base-8192-multi-tok-new-procedure-50-epochs-notags\",\n",
    "    \"/home/david/modernbert_chemistry/fineweb/fine-web-modernbert-base-8192-multi-tok-new-procedure-60-epochs-notags\",\n",
    "\n",
    "]\n",
    "\n",
    "model_titles = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"ChemBERTa-MLM\",\n",
    "    \"ChemBERTa-MTR\",\n",
    "    \"MoLFormer\",\n",
    "    \"SciBERT_uncased\",\n",
    "    \"SciBERT_cased\",\n",
    "    \"ModernBERT (MB)\",\n",
    "    # \"ModernBERT base 500k\",\n",
    "    # \"ModernBERT base Procedures 500k\",\n",
    "    # \"ModernBERT base Procedures 1M5 5 epochs\",\n",
    "    # \"ModernBERT base Procedures 1M5 10 epochs\",\n",
    "    # \"ModernBERT base Procedures 1M5 20 epochs\",\n",
    "    # \"ModernBERT base Procedures 1M5 30 epochs\",\n",
    "    # \"ModernBERT base Procedures 1M5 40 epochs\",\n",
    "    # \"ModernBERT base Procedures 1M5 50 epochs\",\n",
    "    \"ModernBERT base Procedures 1M5 60 epochs\"\n",
    "]\n",
    "\n",
    "\n",
    "def get_cls_embeddings(tokenizer, model, texts, batch_size=32, max_length=512):\n",
    "    \"\"\"Extract CLS embeddings for a list of texts.\"\"\"\n",
    "    all_embeddings = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i : i + batch_size]\n",
    "            inputs = tokenizer(batch, padding=\"longest\", truncation=True,\n",
    "                               return_tensors=\"pt\", max_length=max_length)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "            cls_embeds = outputs.hidden_states[-1][:, 0, :].cpu().numpy()\n",
    "            all_embeddings.append(cls_embeds)\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "n_experiments = 3\n",
    "all_results_reg = {}\n",
    "\n",
    "for model_dir, title in zip(model_dirs, model_titles):\n",
    "    print(f\"\\nModel: {title} \")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_dir, output_hidden_states=True)\n",
    "    model.to(device)\n",
    "    \n",
    "    model_tasks = {}\n",
    "    for task_name, (df_task, label_col) in datasets.items():\n",
    "        print(f\"Task: {task_name}\")\n",
    "        # Prepare data\n",
    "        texts = df_task[\"SMILES\"].astype(str).tolist()\n",
    "        y_cont = df_task[label_col].astype(float).values\n",
    "        \n",
    "        X_embeds = get_cls_embeddings(tokenizer, model, texts)\n",
    "        \n",
    "        r2_scores, mse_scores = [], []\n",
    "        for run in range(n_experiments):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_embeds, y_cont, test_size=0.2, random_state=42 + run\n",
    "            )\n",
    "            reg = RandomForestRegressor(\n",
    "                n_estimators=100,\n",
    "                random_state=42 + run,\n",
    "                n_jobs=-1     # <- use all cores\n",
    "            )\n",
    "            # reg = XGBRegressor(\n",
    "            #     n_estimators=100,\n",
    "            #     random_state=42 + run,\n",
    "            #     objective=\"reg:squarederror\",  # standard regression objective\n",
    "            #     verbosity=0\n",
    "            # )\n",
    "            reg.fit(X_train, y_train)\n",
    "            preds = reg.predict(X_test)\n",
    "            r2  = r2_score(y_test, preds)\n",
    "            mse = mean_squared_error(y_test, preds)\n",
    "            r2_scores.append(r2)\n",
    "            mse_scores.append(mse)\n",
    "        mean_r2  = np.mean(r2_scores)\n",
    "        std_r2   = np.std(r2_scores)\n",
    "        mean_mse = np.mean(mse_scores)\n",
    "        std_mse  = np.std(mse_scores)\n",
    "        \n",
    "        print(f\"  R²:  {mean_r2:.3f} ± {std_r2:.3f}\")\n",
    "        print(f\"  MSE: {mean_mse:.3f} ± {std_mse:.3f}\")\n",
    "        \n",
    "        model_tasks[task_name] = {\n",
    "            \"r2_scores\": r2_scores,\n",
    "            \"mse_scores\": mse_scores,\n",
    "            \"mean_r2\": mean_r2,\n",
    "            \"std_r2\": std_r2,\n",
    "            \"mean_mse\": mean_mse,\n",
    "            \"std_mse\": std_mse\n",
    "        }\n",
    "    \n",
    "    all_results_reg[title] = model_tasks\n",
    "\n",
    "rows = []\n",
    "for model, tasks in all_results_reg.items():\n",
    "    for task, metrics in tasks.items():\n",
    "        rows.append({\n",
    "            \"model\": model,\n",
    "            \"task\": task,\n",
    "            \"mean_r2\": metrics[\"mean_r2\"],\n",
    "            \"std_r2\": metrics[\"std_r2\"],\n",
    "            \"mean_mse\": metrics[\"mean_mse\"],\n",
    "            \"std_mse\": metrics[\"std_mse\"]\n",
    "        })\n",
    "\n",
    "results_df_reg = pd.DataFrame(rows)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
